## 集群安装

<img src="images/截屏2022-03-05 18.43.49.png" alt="截屏2022-03-05 18.43.49" style="zoom: 30%;" />

在 mapred-site-xml中插入以下代码，HADOOP_HOME写绝对路线有效

```xml
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=/opt/module/hadoop3.3.0</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=/opt/module/hadoop3.3.0</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=/opt/module/hadoop3.3.0</value>
</property>
```

## 远程连接hive

### 问题

`Hadoop`集群、`hiveserver2`已正常启动，`datagrip`连接报错`org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: zzy is not allowed to impersonate zzy org.apache.hadoop.ipc.RemoteException:User: zzy is not allowed to impersonate zzy`

### 解决

在 Hadoop的 core-site.xml 文件添加如下内容

```
<property>
   <name>hadoop.proxyuser.zzy.hosts</name>
   <value>*</value>
   <description>
        The 'nfsserver' user is allowed to proxy all members of the 'users-group1' and
       'users-group2' groups. Note that in most cases you will need to include the
        group "root" because the user "root" (which usually belonges to "root" group) will
        generally be the user that initially executes the mount on the NFS client system.
        Set this to '*' to allow nfsserver user to proxy any group.
    </description>
</property>
<property>
       <name>hadoop.proxyuser.zzy.groups</name>
       <value>*</value>
       <description>
               This is the host where the nfs gateway is running. Set this to '*' to allow
               requests from any hosts to be proxied.
       </description>
</property>
```

将下面两个目录给上777权限

```shell
hadoop fs -chmod 777 /tmp
hadoop fs -chmod 777 /user/hive/warehouse
```

重启`hadoop`集群和`hiveserver2`

## hive on spark找不到LZO

### 问题

查询使用`INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'`的表时报`Failed with exception java.io.IOException:java.io.IOException: No LZO codec found, cannot run.`

并且`$HADOOP_HOME/share/hadoop/common/`，`$HIVE_HOME/lib/`，集群的`/spark-jars/`目录下都有`hadoop-lzo-0.4.20.jar`文件，`$HADOOP_HOME/etc/hadoop/core-site.xml`里也配置了`lzo`相关配置:

```
<configuration>
    ...
    <property>
        <name>io.compression.codecs</name>
        <value>
            org.apache.hadoop.io.compress.GzipCodec,
            org.apache.hadoop.io.compress.DefaultCodec,
            org.apache.hadoop.io.compress.BZip2Codec,
            org.apache.hadoop.io.compress.SnappyCodec,
            com.hadoop.compression.lzo.LzoCodec,
            com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>
    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
</configuration>
```

### 解决

**此错误是由于 hive 读取错误的 core-site.xml造成。**节点安装了旧版本的 `hadoop`，也安装了 `hbase`，环境变量也正常配置。但是`hive`脚本读取了 `$HBASE_HOME/conf/core-site.xml `下的 `core-site.xml`里面并且没有配置lzo压缩。